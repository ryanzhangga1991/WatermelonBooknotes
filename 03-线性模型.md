## 3 线性模型

### 3.1 基本形式

线性模型，linear model试图通过特征线性组合的方式完成一个学习器，即：
$$
f(x)=w_1x_1+w_2x_2+...+w_dx_d+b
$$
更一般地，我们会选择使用向量表示：
$$
f(x)=w^Tx+b
$$
线性模型结构简单，但蕴含着机器学习的基本思想，许多复杂的非线性模型都可以在线性模型的基础上，通过层级结果或高维映射得到。

另外，由于权重直观表示了各个特征对预测结果的重要性，因此线性模型一般具有非常好的可解释性。

### 3.2 线性回归

线性回归试图学得，
$$
f(x_i)=wx_i+b,使得  f(x_i)\rarr y_i
$$
我们一般选择均方误差MSE作为lr的损失函数，因此我们使其最小化，
$$
loss(w,b) =argmin\sum_{i=1}^{m}(f(x_i)-y_i)^2=argmin\sum_{i=1}^{m}(wx_i+b-y_i)^2
$$
均方误差有很好的几何意义，本质上是一种欧式距离。我们可以用最小二乘法来寻找均方误差最小的解。

对loss分别求偏导，
$$
\frac{\partial loss(w,b) }{\partial w}=2(\sum_{i=1}^m(wx_i^2+(b-y_i)x_i))\\
\frac{\partial loss(w,b) }{\partial b}=2(mb-\sum_{i=1}^m(y_i-wx_i))\\
$$
另上式为0，就可以得到w和b的最优解，
$$
b=\frac{1}{m}\sum_{i=1}^m(y_i-wx_i)\\
w=\frac{\sum_{i=1}^my_i(x_i-\overline x)}{\sum_{i=1}^mx_i^2-\frac{1}{m}(\sum_{i=1}^mx_i)^2}
$$

### 3.3 Logistic 回归

很多时候，我们通过简单地改造线性回归函数，可以实现非线性的结果。

对数几率回归是一种代表性的广义线性模型，考虑单调可微函数g，可令对数几率回归如下，
$$
y=g(w^Tx+b)
$$
在二分类任务中，对预测值转化为0/1标签时候，最理想的是使用**单位阶跃函数**，unit-step，
$$
y=
\begin{cases}
0,\quad z< 0\\
0.5, \quad z =0 \\
1 ,\quad z>0
\end{cases}
$$
显然单位阶跃函数不连续，因此不可微。我们最理想的方法是寻找它的替代函数，满足单调可微，最常见的替代函数就是sigmoid function：
$$
y=\frac{1}{1+e^{-z}}
$$
<img src="https://raw.githubusercontent.com/ryanzhangga1991/img_cache/main/uPic/image-20230616142239119.png" alt="image-20230616142239119" style="zoom:50%;" />

代入后得到，
$$
y=\frac{1}{1+e^{-(w^Tx+b)}}
$$
变换后可得，
$$
\ln\frac{1}{1-y}=w^x+b
$$
若将y是做正，1-y为负，则两者的比值称作几率（odds），反映了x作为正例的相对可能性，显然$\ln\frac{1}{1-y}$就是对数几率（log odds，简称logits，逻辑斯特回归的取名由来）

虽然，Logistic Rregression名字叫做线性回归，本质上是一种经过非线性激活的分类器。

### 3.4 LDA

线性判别分析（LDA, Linear Discriminant Analysis），又称作 Fisher 判别分析。

LDA的思想非常朴素，给定训练样本集，并设法把他们投影到一条直线上，使得同类样本投影点尽可能近，异类样本点尽可能远；在对新样本进行分类时，讲起投影到同样的线上，就可以根据投影点的位置判断新样本的类别。

<img src="https://raw.githubusercontent.com/ryanzhangga1991/img_cache/main/uPic/image-20230616144247049.png" alt="image-20230616144247049" style="zoom:50%;" />

**所以，LDA的本质是是一种单一维度的表征学习器。**

定义数据集D，并定义$X_i、\mu_i、\sum_i$为示例的集合、均值向量、协方差矩阵。

若数据投影到向量$w$上，我们可以得到两类关键度量指标

- 中心点投影分别为 $w^T\mu_0、w^T\mu_1$；
- 协方差分别为 $w^T\sum_0w、w^T\sum_1w$；

由于我们是二分类，向量是一维空间，因此上述度量均为实数。

再重新看Fisher对LDA的定义，要求同类投影点尽可能在近，其实就是要让 $w^T\sum_0w+w^T\sum_1w$ 尽可能小；要求异类投影点尽可能远，其实就是要让 $||w^T\mu_0-w^T\mu_1||_2$ 模尽量大，因此可以得到目标函数并使之最大化：
$$
J=\frac{||w^T\mu_0-w^T\mu_1||_2^2}
{w^T\sum_0w+w^T\sum_1w}\\
=\frac{w^T(\mu_0-\mu_1)(\mu_0-\mu_1)^Tw}
{w^T(\sum_0+\sum_1)w}
$$
定义类内散度矩阵（within-class scatter matrix）:
$$
S_w=\sum_0+\sum_1
$$
定义类间散度矩阵（between-class matrix）：
$$
S_b=(\mu_0-\mu_1)(\mu_0-\mu_1)^T
$$
由于分子分母都是w的二次项，因此解与w的模长无关，只与其方向有关，为了不是一般性，我们可以先令$w^TS_ww=1$，目标简化为，
$$
min_w\quad-w^TS_bw\\
s.t.\quad w^TS_ww=1
$$
使用拉格朗日函数求解，上式又可以等价于
$$
L(w,\lambda)=-w^TS_bw+\lambda(w^TS_ww-1)
$$
我们对$w$求偏导可以得到，
$$
\frac{\partial L(w,\lambda)}{\partial w}=-
$$






























