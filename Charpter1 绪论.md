## Charpter1 绪论

### 1.1 引言

机器学习所研究的主要内容，是关于计算机上从数据中产生“模型”的算法，即学习算法（learning algorithm）。通过学习算法，我们可以将数据提供给模型，并生成相应的判断。

### 1.2 基本书语

- 数据集（dataset）：数据记录的集合
- 示例（instance）：单条记录
- 样本（sample)：与「示例」同义
- 属性（attribute）：反映事件或对象在某方面的表现或性质
- 特征（feature）：与「属性」同义

### 1.3 假设空间

归纳（induction）与演绎（deduction）是科学推理的两大手段，前者从特殊到一般的泛化过程（generlization），后者为一般到特殊的特化过程（specialization）。因此，演绎一般是基于一组公式定理推导出与之相恰的规则；机器学习是「从样本组中学习」，显然是归纳的过程，因此称作归纳学习（inductive learning）。

我们把学习过程看做一个在所有假设（hypothesis）组成的空间中进行搜索的过程， 搜索的目的是找到与训练集合匹配的假设。假设空间（hypothesis space）就是包含这一系列假设的集合。我们可以有许多的方法对假设空间进行搜索，例如自上而下、一般到特殊、正反例等。

在现实问题中，可以有多个假设与训练集一致，这些假设的集合就称作版本空间（version space）。

### 1.4 归纳偏好

针对某个版本空间，不同的算法会有不同归纳偏好（inductive bias），具体表现为对某种类型的假设或特征的特殊偏好。

任何一个有效的机器学习算法必然存在特定的归纳偏好，否则它将被假设空间中看起在训练集合上“等效”的假设迷惑，而无法产生确定的学习结果。如何理解上文呢？我认为比较简单的理解方法是，不同的机器学习方法更新算法和估量损失的方法必然自成体系，而这种方法必然带有某种的偏好。

Question，有没有一种方法帮助我们选择更好的学习方法？

**"奥卡姆剃刀" （Occam's razor）是一种常用的自然科学研究的基本原则，即“如无必要，勿增实体”。**因此学习曲线更加“平滑”的算法，有可能是鲁棒性更强的。







