## 2 模型评估与选择

### 2.1 经验误差与过拟合

几个概念：

- 错误率，error rate.  E=err_num/num

- 精度，accruacy. A=1-E

- 误差，error

  学习器的实际预测输出与样本的真实输出之间的差异

- 经验误差，empirical error

  学习器在训练集上的误差。

  又称作训练误差，training error

- 泛化误差，generalization error

  学习器在新样本上的误差

- 过拟合，overfitting

  当学习器把训练样本学得"太好"了的时候，很可能已经把训练样本自身的一些特点当作了所有潜在样本都 会具有的一般性质，这样就会导致泛化性能下降

### 2.2 评估方法

使用一个 "测试集" (testing set)来测试学习器对新样本的判别能力，然后以测试集上的"测试误差" (testingerror)作为泛化误差的近似。

#### 2.2.1 Hold Out

"留出法" (hold out)将数据集 D 划分为两个互斥的集合，其中一个 集合作为训练集，另一个作为测试集。

最常用的方法就是 sklearn.train_test_splite()

#### 2.2.2 Cross Validation

"交叉验证法" (cross validation)先将数据集D划分为k个大小相似的互斥子集，每个子集都尽可能保持数据分布的一致性。

每次用 k-1 个子集的并集作为训练集余下F的那个子集作为测试集，这样就可获得 k 组训练/测试集，从而可进行 k次训练和测试，最终返回的是这 k个测试结果 的均值。

显然，交叉验证法评估结果的稳定性和保真性在很大程度上取决于 k 的取值，为强调这一点，通常把交叉验证法称为 " k 折交叉验证" (k-fold cross validation)。

![image-20230612180439323](https://raw.githubusercontent.com/ryanzhangga1991/img_cache/main/uPic/image-20230612180439323.png)

#### 2.2.3 Bootstrap

自助法(Bootstrap)的本意是「解靴带」，引自德国「吹牛大王历险记」中解靴自助的典故，因此代表的是自助法。

自助采样法(bootstrap sampling)是，给定包含 m 个样本的数据集 D:

- 每次随机从 D 中挑选一个 样本，将其拷贝放入 D' 
- 再将该样本放回初始数据集 D 中，使得该样本在下次采样时仍有可能被采到
- 这个过程重复执行 m 次

我们就得到了包含 m 个样本的数据集  D' 。

简单估计，某样本在m次采样中始终未被猜到的概率：
$$
\lim_{m\rarr\infin}(1-\frac{1}{m})^m\rarr\frac{1}{e}\approx0.368
$$
即通过自助采样，当m足够大时，D中约有36.8%的数据未出现在采样数据集D'中，这样实际评估的模型与期望评估的模型均使用m个训练样本，而仍有可观数量的数据可以用于测试。

这样的测试结果，称作「包外估计」out-of-bag-estimate

bootstrap因为可以产生任意数量训练集，常见于集成学习，但由于D'改变了数据集的原始分布，引入了估计偏差，因此在D足够大的时候，hold-out和cross validation会更加常用一些。

#### 2.2.4 Parameter tuning

在模型评估与选择时，除了对使用学习算法的选择外，还需要对算法参数进行设定，这就是常说的「参数调节」，亦称作调参。

调参和算法选择没什么本质区别:对每种参数配置都 训练出模型，然后把对应最好模型的参数作为结果。

### 2.3 性能度量

性能度量，Performance measure，要评估学习器的性能，就要把预测结果与真实标记进行比较，不同的评估方法，会得到不同的「好坏」结果，因此模型的好坏严格意义上来说是相对的，模型的能力不仅仅取决于使用的算法、数据，也取决于任务需求。

这里假设学习器为函数 $f(x)$.

回归分析中，最常用的性能度量是均方误差 MSE（mean squared error）：
$$
MSE = \frac{1}{m}\sum_{i=1}^{m}(f(x_i)-y_i)^2
$$
更一般地，对于数据分布D和概率分布p，MSE可以如下表示：
$$
MSE=\int_D(f(x)-y)^2p(x)dx
$$

#### 2.3.1 错误率与准确率

错误率与准确率是分类任务(classify)中常用的性能度量，可以用于二分类、多分类。

错误率可定义为：
$$
error = \frac{1}{m}\sum_{i=1}^msig(f(x_i)\neq y_i)
$$

$$
error=\int_Dsig(f(x)\neq y)p(x)dx
$$



准确率可定义为：
$$
acc=\frac{1}{m}\sum_{i=1}^msig(f(x_i)=y_i)=1-error
$$

$$
 acc=\int_Dsig(f(x)= y)p(x)dx
$$

其中$sgn()$是指示函数，若为真则取值 1，否则取值 0.

#### 2.3.2 精准率、召回率与 F1-score、混淆矩阵

虽然错误率与准确率是非常直观描述分类任务结果的指标，但大多数时候分类任务的衡量指标会更加综合。

针对二分类问题，可以根据真实类别与学习器预测类别组合得到如下混淆矩阵：

|          |          预测结果           | 预测结果 |
| :------: | :-------------------------: | :------------------------: |
| 真实情况 |            正例             |            反例            |
|   真例   | 真正例（TP,True Positive）  | 假反例（FN,False Negtive） |
|   假例   | 假正例（FP,False Positive） | 真反例（TN,True Negtive）  |

根据混淆矩阵，可以定义精准率（Precision）和召回率（Recall）

精准率，代表的预测为正的结果中有多少验证为真。
$$
Precision=\frac{TP}{TP+FP}
$$
召回率，代表的是真样本数据集中有多少被预测为正。
$$
Recall=\frac{TP}{TP+FN}
$$
**精准率和召回率是一对矛盾的度量指标**，以二分类器举例：

- 若希望精准率足够高，那么就需要提高判断阈值，那么预测的正样本大概率都会是真例。
- 提高阈值后，会筛去很多不确定的样本，中间必然导致漏掉真例，那么召回率就一定会下去。

为了克服上述矛盾，可以通过绘制“Precision-Recall图”寻找平衡点：

![image-20230615143425629](https://raw.githubusercontent.com/ryanzhangga1991/img_cache/main/uPic/image-20230615143425629.png)

学习器A\B\C的性能可以根据曲线包含的面积大小判断性能优越程度，可以看到学习器A的曲线完全覆盖了学习器C，因此可以判断学习器A优于学习器C。

但学习器A\B之间很难直接用面积大小来判断，这时候就提出了平衡点（BEP，Break-Event Point），它代表的是 P=R时候的取值。如上图中，A的BEP高于B，就可以判断A比B性能更好。

而在机器学习中，更常用的度量方式是F1，即精准率和召回率的调和平均
$$
F1=\frac{1}{2}(\frac{1}{Precision}+\frac{1}{Recall})=\frac{2\times Precision \times Recall}{Precision + Recall}
$$
为什么这里选择使用调和平均呢？因为相较算数平均$\frac{P+R}{2}$和几何平均$\sqrt{P\times R}$，调和平均更加重视较小值。

F1这里的1其实是一个超参数，代表了精准率和召回率的相对重要性。更一般的形式如下：
$$
F_\beta=\frac{1}{1+\beta^2}(\frac{1}{P}+\frac{\beta^2}{R})=\frac{(1+\beta^2)\times P\times R}{\beta^2 \times P +R}
$$
可以看到$\beta$在[0,1]区间的时候，Precision有更大影响；大于1时，Recall对F值影响更大。

#### 2.3.3 ROC 与 AUC

ROC的全称是“受试者工作特征”（Receiver Operating Characteristic），源于二战中敌机雷达信号分析技术。

与P-R曲线不同的是，他的纵坐标为真正率TPR，True Positive Rate
$$
TPR = \frac{TP}{TP+FN}
$$
横坐标为假正率FPR，False Postive Rate
$$
FPR=\frac{FP}{FP+TN}
$$
![image-20230615152036245](https://raw.githubusercontent.com/ryanzhangga1991/img_cache/main/uPic/image-20230615152036245.png)

如果存在多个分类器，ROC曲线无法区分性能的情况下，可以使用AUC（Area Under ROC Curve）来判断，AUC面积越大则性能越好。

#### 2.3.4 代价敏感错误率 与 代价曲线

在现实生活中，不同的错误会导致不同的结果。

最典型的案例：癌症的筛查诊断，FT（假真例）会增加患者的就诊流程；FN（假反例）后果严重，会让患者失去及时就诊的机会。

为了权衡不同类型错误所造成的不同损失，可为错误赋予“非均等代价”（unequal cost）。

以二分类任务为例，可以设定一个代价矩阵（cost matrix）如下：

![image-20230615153759470](https://raw.githubusercontent.com/ryanzhangga1991/img_cache/main/uPic/image-20230615153759470.png)

因此推广到错误率上，带有敏感度的错误率是：
$$
error = \frac{1}{m}(\sum_{D^+}sig(f(x_i)\neq y_i)\times cost_{01} + \sum_{D^-}sig(f(x_i)\neq y_i)\times cost_{10})
$$
上式中，$D^+D^-$代表正例与负例。

### 2.4 比较检验

统计假设检验（hypothesis test）为学习器性能比较提供了重要依据。

若在test set上观察到学习器A优于学习器B，则我们需要知道A的泛化心梗是否在统计意义上优于B，以及这个结论的把握有多大。

本节默认以错误率作为度量指标，并用$\epsilon$表示。

#### 2.4.1 假设检验

“假设”是对错误率分布的一种猜想，我们并不知道学习器泛化错误率$\epsilon$，只能知道其测试错误率$\hat\epsilon$。两者直观上相近的可能性较大。

泛化错误率$\epsilon$的学习器在一个样本上错误的概率是$\epsilon$；测试错误率$\hat\epsilon$意味着在测试样本（m个）中恰好有$m \times \hat\epsilon$个误判样本。

- 假设测试集从样本总体中独立采样得到

那么学习器将其中m'个样本误判的概率为 $\epsilon^{m'}(1-\epsilon)^{m-m'}$；由此可知恰好将 $\hat\epsilon\times m$个样本误判的概率如下表示，这个概率恰好也是泛化错误率为$\epsilon$的学习器在测试集得到测试错误率$\hat\epsilon$的的概率；
$$
P(\hat\epsilon|\epsilon)=\begin{pmatrix}m\\ \hat\epsilon\times m\end{pmatrix}\epsilon^{\hat\epsilon\times m}(1-\epsilon)^{m-\hat\epsilon\times m}
$$
其中括号代表组合数，
$$
\begin{pmatrix}m\\ \hat\epsilon\times m\end{pmatrix}=\frac{m!}{(\hat\epsilon\times m)!(m-\hat\epsilon\times m)!}=C_{m}^{\hat\epsilon\times m}
$$
对$\epsilon$进行一阶求导，
$$
\frac{\partial P(\hat\epsilon|\epsilon)}{\partial\epsilon}
=\begin{pmatrix}m\\ \hat\epsilon\times m\end{pmatrix}
\frac{\epsilon^{\hat\epsilon\times m}(1-\epsilon)^{m-\hat\epsilon\times m}}{\partial\epsilon}\\
=\begin{pmatrix}m\\ \hat\epsilon\times m\end{pmatrix}
(\hat\epsilon\times m\times\epsilon^{\hat\epsilon\times m -1}(1-\epsilon)^{m-\hat\epsilon\times m}+\epsilon^{\hat\epsilon\times m}\times (m-\hat\epsilon\times m)\times (1-\epsilon)^{m-\hat\epsilon\times m-1}\times(-1))\\
=\begin{pmatrix}m\\ \hat\epsilon\times m\end{pmatrix}
\epsilon^{\hat\epsilon\times m-1}(1-\epsilon)^{m-\hat\epsilon\times m-1}\times
(\hat\epsilon\times m\times(1-\epsilon)-\epsilon\times(m-\hat\epsilon\times m))
\\
=\begin{pmatrix}m\\ \hat\epsilon\times m\end{pmatrix}
\epsilon^{\hat\epsilon\times m-1}(1-\epsilon)^{m-\hat\epsilon\times m-1}\times
(\hat\epsilon\times m-\epsilon\times m)
$$
在$\hat\epsilon=\epsilon$时，P取到最大值

可以证明我们的直觉：“两者直观上相近的可能性较大。”

后面讲了假设检验、t检验和大量研究生时期的统计学检验方法的过程，由于和我NLP相关的工作距离较远，我选择了跳过（统计苦手

### 2.5 偏差与方差

算法泛化性能可以通过“偏差-方差分解”（bias-variance decompositon）解释。

学习器在不同训练集上的结果很可能不同，假设

学习算法的期望输出为，
$$
\overline f(x)=\Epsilon_D[f(x)]
$$
使用相同样本数产生的方差variance为，
$$
var(x)=\Epsilon_D[(f(x)-\overline f(x))^2]
$$
噪声为，
$$
\epsilon^2=\Epsilon_D[(y_D-y)^2]
$$
期望输出与真实标记的偏差bias为，
$$
bias^2(x)=(\overline f(x)-y)^2
$$
经过推导可以知道，
$$
error =bias^2(x)+var(x)+\epsilon^2
$$
泛化误差可分解为偏差、方差与噪声之和。

下面是 http://scott.fortmann-roe.com/docs/BiasVariance.html

对泛化误差中，偏差与方差的理解

![image-20230616111113852](https://raw.githubusercontent.com/ryanzhangga1991/img_cache/main/uPic/image-20230616111113852.png)

假设红色的靶心区域是真例区，蓝色点为模型预测值。

比较集中的属于方差比较小，比较分散的属于方差比较大的情况。

靠近红色靶心的属于偏差较小的情况，远离靶心的属于偏差较大的情况。

因此，方差小代表了模型泛化能力强输出稳定，偏差小代表模型精准。

#### 2.5.1 偏差-方差窘境

偏差-方差窘境 (bias-variance dilemma)指的是两者目标的冲突。

<img src="https://raw.githubusercontent.com/ryanzhangga1991/img_cache/main/uPic/image-20230616111512961.png" alt="image-20230616111512961" style="zoom:50%;" />

假定我们能控制学习算法的训练程度，则：

- 在训练不足时，学习器的拟合能力不够强，训练数据的扰动不足以使学习器产生显著变化，此时偏差主导了泛化错误率

  比如，此时正在学习用直线区分二分类，偏差偏大

- 随着训练程度的加深， 学习器的拟合能力逐渐增强，训练数据发生的扰动渐渐能被学习器学到，方差逐渐主导了泛化错误率

  比如，此时正在学习使用非常复杂的曲线做二分类，方差变得很大

- 在训练程度充足后，学习器的拟合能力已非常强，训练数据发生的轻微扰动都会导致学习器发生显著变化，若训练数据自身的、非全局的特性被学习器学到了，则将发生过拟合
