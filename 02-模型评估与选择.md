## 2 模型评估与选择

### 2.1 经验误差与过拟合

几个概念：

- 错误率，error rate.  E=err_num/num

- 精度，accruacy. A=1-E

- 误差，error

  学习器的实际预测输出与样本的真实输出之间的差异

- 经验误差，empirical error

  学习器在训练集上的误差。

  又称作训练误差，training error

- 泛化误差，generalization error

  学习器在新样本上的误差

- 过拟合，overfitting

  当学习器把训练样本学得"太好"了的时候，很可能已经把训练样本自身的一些特点当作了所有潜在样本都 会具有的一般性质，这样就会导致泛化性能下降

### 2.2 评估方法

使用一个 "测试集" (testing set)来测试学习器对新样本的判别能力，然后以测试集上的"测试误差" (testingerror)作为泛化误差的近似。

#### 2.2.1 Hold Out

"留出法" (hold out)将数据集 D 划分为两个互斥的集合，其中一个 集合作为训练集，另一个作为测试集。

最常用的方法就是 sklearn.train_test_splite()

#### 2.2.2 Cross Validation

"交叉验证法" (cross validation)先将数据集D划分为k个大小相似的互斥子集，每个子集都尽可能保持数据分布的一致性。

每次用 k-1 个子集的并集作为训练集余下F的那个子集作为测试集，这样就可获得 k 组训练/测试集，从而可进行 k次训练和测试，最终返回的是这 k个测试结果 的均值。

显然，交叉验证法评估结果的稳定性和保真性在很大程度上取决于 k 的取值，为强调这一点，通常把交叉验证法称为 " k 折交叉验证" (k-fold cross validation)。

![image-20230612180439323](https://raw.githubusercontent.com/ryanzhangga1991/img_cache/main/uPic/image-20230612180439323.png)

#### 2.2.3 Bootstrap

自助法(Bootstrap)的本意是「解靴带」，引自德国「吹牛大王历险记」中解靴自助的典故，因此代表的是自助法。

自助采样法(bootstrap sampling)是，给定包含 m 个样本的数据集 D:

- 每次随机从 D 中挑选一个 样本，将其拷贝放入 D' 
- 再将该样本放回初始数据集 D 中，使得该样本在下次采样时仍有可能被采到
- 这个过程重复执行 m 次

我们就得到了包含 m 个样本的数据集  D' 。

简单估计，某样本在m次采样中始终未被猜到的概率：
$$
\lim_{m\rarr\infin}(1-\frac{1}{m})^m\rarr\frac{1}{e}\approx0.368
$$
即通过自助采样，当m足够大时，D中约有36.8%的数据未出现在采样数据集D'中，这样实际评估的模型与期望评估的模型均使用m个训练样本，而仍有可观数量的数据可以用于测试。

这样的测试结果，称作「包外估计」out-of-bag-estimate

bootstrap因为可以产生任意数量训练集，常见于集成学习，但由于D'改变了数据集的原始分布，引入了估计偏差，因此在D足够大的时候，hold-out和cross validation会更加常用一些。

#### 2.2.4 Parameter tuning

在模型评估与选择时，除了对使用学习算法的选择外，还需要对算法参数进行设定，这就是常说的「参数调节」，亦称作调参。

调参和算法选择没什么本质区别:对每种参数配置都 训练出模型，然后把对应最好模型的参数作为结果。

### 2.3 性能度量

性能度量，Performance measure，要评估学习器的性能，就要把预测结果与真实标记进行比较，不同的评估方法，会得到不同的「好坏」结果，因此模型的好坏严格意义上来说是相对的，模型的能力不仅仅取决于使用的算法、数据，也取决于任务需求。

这里假设学习器为函数 $f(x)$.

回归分析中，最常用的性能度量是均方误差 MSE（mean squared error）：
$$
MSE = \frac{1}{m}\sum_{i=1}^{m}(f(x_i)-y_i)^2
$$
更一般地，对于数据分布D和概率分布p，MSE可以如下表示：
$$
MSE=\int_D(f(x)-y)^2p(x)dx
$$

#### 2.3.1 错误率与准确率

错误率与准确率是分类任务(classify)中常用的性能度量，可以用于二分类、多分类。

错误率可定义为：
$$
error = \frac{1}{m}\sum_{i=1}^msig(f(x_i)\neq y_i)
$$

$$
error=\int_Dsig(f(x)\neq y)p(x)dx
$$



准确率可定义为：
$$
acc=\frac{1}{m}\sum_{i=1}^msig(f(x_i)=y_i)=1-error
$$

$$
 acc=\int_Dsig(f(x)= y)p(x)dx
$$

其中$sgn()$是指示函数，若为真则取值 1，否则取值 0.

#### 2.3.2 精准率、召回率与 F1-score、混淆矩阵

虽然错误率与准确率是非常直观描述分类任务结果的指标，但大多数时候分类任务的衡量指标会更加综合。

针对二分类问题，可以根据真实类别与学习器预测类别组合得到如下混淆矩阵：

|          |          预测结果           | 预测结果 |
| :------: | :-------------------------: | :------------------------: |
| 真实情况 |            正例             |            反例            |
|   真例   | 真正例（TP,True Positive）  | 假反例（FN,False Negtive） |
|   假例   | 假正例（FP,False Positive） | 真反例（TN,True Negtive）  |

根据混淆矩阵，可以定义精准率（Precision）和召回率（Recall）

精准率，代表的预测为正的结果中有多少验证为真。
$$
Precision=\frac{TP}{TP+FP}
$$
召回率，代表的是真样本数据集中有多少被预测为正。
$$
Recall=\frac{TP}{TP+FN}
$$
**精准率和召回率是一对矛盾的度量指标**，以二分类器举例：

- 若希望精准率足够高，那么就需要提高判断阈值，那么预测的正样本大概率都会是真例。
- 提高阈值后，会筛去很多不确定的样本，中间必然导致漏掉真例，那么召回率就一定会下去。

为了克服上述矛盾，可以通过绘制“Precision-Recall图”寻找平衡点：

![image-20230615143425629](https://raw.githubusercontent.com/ryanzhangga1991/img_cache/main/uPic/image-20230615143425629.png)

学习器A\B\C的性能可以根据曲线包含的面积大小判断性能优越程度，可以看到学习器A的曲线完全覆盖了学习器C，因此可以判断学习器A优于学习器C。

但学习器A\B之间很难直接用面积大小来判断，这时候就提出了平衡点（BEP，Break-Event Point），它代表的是 P=R时候的取值。如上图中，A的BEP高于B，就可以判断A比B性能更好。

而在机器学习中，更常用的度量方式是F1，即精准率和召回率的调和平均
$$
F1=\frac{1}{2}(\frac{1}{Precision}+\frac{1}{Recall})=\frac{2\times Precision \times Recall}{Precision + Recall}
$$
为什么这里选择使用调和平均呢？因为相较算数平均$\frac{P+R}{2}$和几何平均$\sqrt{P\times R}$，调和平均更加重视较小值。

F1这里的1其实是一个超参数，代表了精准率和召回率的相对重要性。更一般的形式如下：
$$
F_\beta=\frac{1}{1+\beta^2}(\frac{1}{P}+\frac{\beta^2}{R})=\frac{(1+\beta^2)\times P\times R}{\beta^2 \times P +R}
$$
可以看到$\beta$在[0,1]区间的时候，Precision有更大影响；大于1时，Recall对F值影响更大。

#### 2.3.3 ROC 与 AUC

ROC的全称是“受试者工作特征”（Receiver Operating Characteristic），源于二战中敌机雷达信号分析技术。

与P-R曲线不同的是，他的纵坐标为真正率TPR，True Positive Rate
$$
TPR = \frac{TP}{TP+FN}
$$
横坐标为假正率FPR，False Postive Rate
$$
FPR=\frac{FP}{FP+TN}
$$
![image-20230615152036245](https://raw.githubusercontent.com/ryanzhangga1991/img_cache/main/uPic/image-20230615152036245.png)

如果存在多个分类器，ROC曲线无法区分性能的情况下，可以使用AUC（Area Under ROC Curve）来判断，AUC面积越大则性能越好。

#### 2.3.4 代价敏感错误率 与 代价曲线

在现实生活中，不同的错误会导致不同的结果。

最典型的案例：癌症的筛查诊断，FT（假真例）会增加患者的就诊流程；FN（假反例）后果严重，会让患者失去及时就诊的机会。

![image-20230615153759470](https://raw.githubusercontent.com/ryanzhangga1991/img_cache/main/uPic/image-20230615153759470.png)
